{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clusterability","provenance":[{"file_id":"1LdAVy1Kcg2EhYkARHKsZgt-z8ffLa01Q","timestamp":1593625373486},{"file_id":"1c3Kb1-VoVGWcren7QQBvShjn1pbv4Ltf","timestamp":1593622979501}],"mount_file_id":"1gyGeh0F4yDfsrbw84CptHzlgh_1EtXmq","authorship_tag":"ABX9TyODayAApr85Jy8N5YrrNXR1"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IuM4nte0wxmv","colab_type":"text"},"source":["# Spatial Histogram"]},{"cell_type":"code","metadata":{"id":"4LF3RIj5htmc","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA\n","import pandas as pd\n","import numpy as np\n","import scipy as sp\n","from scipy.stats import entropy\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5_tqurE8vFZ","colab_type":"code","colab":{}},"source":["# Spatial Histogram for Clustering Tendency\n","def spaHist(arr, bins=5, n = 500):\n","    '''\n","        input: arr: an numpy array of input data in d dimension\n","               bins: the number of bins for computing Estimated Probability Mass Function along dimensions\n","               n: number of random instances for comparison\n","        return:  an numpy array of n KL divergence numbers between the EPMF of the \n","                 input arr and the EPMFs of n randomly generated arrays \n","    '''\n","    \n","    ans = np.zeros(n)\n","\n","    # Compute the Estimated Probability Mass Function for the input array along all its dimensions\n","    # the second paramter number is for the number of binning on a dimension\n","    epmf_input = getEPMF(arr, bins)\n","\n","    for i in range(n):\n","        aRandArr = getRandomArray(arr, arr.shape[0])\n","        epmf_rand = getEPMF(aRandArr, bins)\n","\n","        kl_conv = computeKLConv(epmf_input, epmf_rand)\n","        ans[i] = kl_conv\n","\n","    # Replace the 'inf' KL divergence value with the mean  \n","    kls_vals = np.where(np.isinf(ans), np.mean(ans[np.isfinite(ans)]), ans)\n","\n","    return kls_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJYLrAWvEln4","colab_type":"code","colab":{}},"source":["# Compute the KL divergence between two given EPMF\n","from scipy.stats import entropy\n","def computeKLConv(epmf_input, epmf_rand):\n","    return entropy(epmf_input, epmf_rand, base=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RD4pYQ7n8SDG","colab_type":"code","colab":{}},"source":["# Compute an empirical probability mass function for given array of point\n","def getEPMF(arr, bins=5, smoothing=False):\n","    '''\n","        input: arr: an numpy array of d dimension points\n","               bins: number of bins for computing the EPMF \n","        return: an numpy array of EPMF values in the cells binned along arr's dimensions\n","    '''\n","    dims = arr.shape[1]\n","\n","    # If smoothing is needed, initialize all counts with 1. \n","    ans = np.zeros(int(math.pow(bins, dims)))\n","    if smoothing == True:\n","        ans = np.ones(int(math.pow(bins, dims)))\n","    \n","    # cut each dimension into bins with labels of bin indexes\n","    cats = np.zeros(arr.shape)\n","    for i in range(dims):\n","        cats[:, i] = pd.cut(arr[:, i], bins=bins, labels=range(0, bins))\n","    \n","    # Compute the index of the EPMF array using the \n","    # category numbers of each point in the input array\n","    for i in range(arr.shape[0]):\n","        idx = 0\n","        for j in range(dims):\n","            pow = dims - 1 - j\n","            idx = idx + cats[i, j] * math.pow(bins, pow)\n","        ans[int(idx)] = ans[int(idx)] + 1 # update the counts at the cell indexed by idx\n","\n","\n","    return ans / sum(ans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HoqrWc6FBpKq","colab_type":"code","colab":{}},"source":["# Generate a list of m number of purely random points corresponding to the input array \n","def getRandomArray(arr, m):\n","    '''\n","        input: arr: an input array in d dimensions\n","               m: the number of random points generated\n","        return: an array of m random points in the same dimension as the input array\n","    '''\n","\n","    # The list of minimum values and the list of maximum values\n","    # This two lists define the boundary of the area for randomly generating samples\n","    # We assume the input array has different scales along its dimensions\n","    mins = []\n","    maxs = []\n","\n","    dims = arr.shape[1]\n","\n","    for i in range(dims):\n","        mins.append(arr[:, i].min())\n","        maxs.append(arr[:, i].max())\n","\n","    ans = np.zeros((m, dims))\n","\n","    for i in range(m):\n","        ans[i] = np.random.uniform(mins, maxs)\n","\n","    return ans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugvm1153AMrm","colab_type":"code","colab":{}},"source":["# Generate a random EPMF for testing purpose\n","import numpy.random as random\n","def randEPMF(n = 25):\n","    # random.seed(42)\n","    rnums = random.randint(1, 100, n)\n","       \n","    return rnums / sum(rnums)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43SZmB8U1JBb","colab_type":"text"},"source":["# Hopkins Statistic"]},{"cell_type":"markdown","metadata":{"id":"xYajd4y91Oo8","colab_type":"text"},"source":["The Hopkins statistic is a sparse test for spatial randomness. Given a dataset $\\mathbf{D}$ comprising $n$ points, we generate $t$ random susamples $\\mathbf{R}_{i}$ of $m$ points each, where $m<<n$. These samples are drawn from the same data space as $\\mathbf{D}$, generated uniformly at random along each dimension. Further, we also generate $t$ subsamples of $m$ points directly from $\\mathbf{D}$, using sampling without replacement. "]},{"cell_type":"markdown","metadata":{"id":"pmq88JY3LIdp","colab_type":"text"},"source":["Let $\\mathbf{D}_{i}$ denote the $i$th direct subsample. Next, we compute the minimum distance between each points $\\mathbf{x}_{j}\\in \\mathbf{D}_{i}$ and points in $\\mathbf{D}$\n","\n","$$\\delta_{min}(\\mathbf{x}_{j})=\\min_{\\mathbf{x}_{i}\\in D, \\mathbf{x}_{i}\\neq \\mathbf{x}_{j}}\\left\\{ \\left\\Vert \\mathbf{x}_{j}-\\mathbf{x}_{i} \\right\\Vert \\right\\}$$\n","Likewise, we compute the minimum distance $\\delta_{min}(\\mathbf{y}_{j})$ between a point $\\mathbf{y}_{j}\\in \\mathbf{R}_{i}$ and points in $\\mathbf{D}$."]},{"cell_type":"markdown","metadata":{"id":"mj7hX87gRs0W","colab_type":"text"},"source":["The Hopkins statistic (in $d$ dimensions) for the $i$th pair of samples $\\mathbf{R}_{i}$ and $\\mathbf{D}_{i}$ is then defined as \n","$$ HS_{i}=\\frac{\\Sigma_{\\mathbf{y}_{j}\\in \\mathbf{R}_{i}} (\\delta_{min}(\\mathbf{y}_{j}))^d}{\\Sigma_{\\mathbf{y}_{j}\\in \\mathbf{R}_{i}}(\\delta_{min}(\\mathbf{y}_{j}))^d + \\Sigma_{\\mathbf{x}_{j}\\in \\mathbf{D}_{i}}(\\delta_{min}(\\mathbf{x}_{j}))^d} $$"]},{"cell_type":"markdown","metadata":{"id":"1xUIJKrOVN9r","colab_type":"text"},"source":["This statistic compares the nearest-neighbors distribution of randomly generated points to the same distribution for random subsets of points from $\\mathbf{D}$. If the data is well clustered we expect $\\delta_{min}(\\mathbf{x}_{j})$ values to be smaller compared to the $\\delta_{min}(\\mathbf{y}_{j})$  values, and in this case $HS_{i}$ tends to be 1. If both nearest-neighbor distances are similar, then $HS_{i}$ takes on values close to 0.5, which indicates that the data is essentially random, and there is no apparent clustering. Finally, if $\\delta_{min}(\\mathbf{x}_{j})$ values are larger compared to $\\delta_{min}(\\mathbf{y}_{j})$ values, then $HS_{i}$ tends to 0, and it indicates the point repulsion, with no clustering. From the $t$ different values of $HS_{i}$ we may then compute the mean and variance of the statistic to determne whether $\\mathbf{D}$ is clusterable or not. "]},{"cell_type":"code","metadata":{"id":"6YmGQtRl1-uH","colab_type":"code","colab":{}},"source":["# Generate a subsample of m points directly from the given data set\n","import numpy as np\n","def generateDirectSample(arr, m):\n","    '''\n","        input: arr is an numpy array of data points\n","               m: the size of direct sample without replaclement\n","        return: arr[idxs]: a direct sample of size m from the input numpy array\n","                idxs: the set of random indexes\n","    '''\n","    # number of input data points\n","    n_points = arr.shape[0]\n","    if m > n_points:\n","        raise Exception(\"The required sample size is too large.\")\n","    \n","    idxs = np.random.choice(range(0, n_points), size=m, replace=False)\n","\n","    return arr[idxs], idxs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRHn0a3hSUI2","colab_type":"code","colab":{}},"source":["# Compute the mininum distance from every point in arrA to arrB\n","import numpy as np\n","import scipy as sp\n","def computeMinDistances(arrA, arrB, idxs=None):\n","    '''\n","        input: arrA a set of points in dimension d, typically shorter or equal to arrB\n","               arrB a set of points in dimension d, typically longer than arrA\n","               idxs: a set of indices in arrB which should not be included for computing minimum\n","        return: an array of minimum distances from each point in arrA to arrB\n","    '''\n","    dists = sp.spatial.distance.cdist(arrA, arrB)\n","\n","    if idxs is not None:\n","        n_points = arrA.shape[0]  \n","        dists_ma = np.ma.array(dists, mask=False)\n","        for i in range(n_points):\n","            dists_ma[i, idxs[i]] = True\n","\n","        return np.min(dists_ma, axis=1).data\n","    else:\n","        # return the minimum value of each row (the minimum distance from a point to arrB)\n","        return np.min(dists, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"115AM0UOJDLK","colab_type":"code","colab":{}},"source":["import numpy as np\n","# Compute Hopkins Statistics for a set of points\n","def hopkins(arr, m):\n","    '''\n","        input: arr: a set of points in an numpy arrary in dimention d\n","               m: the size of sample for computing Hopkins Statistics\n","        return: Hopkins Statistics in (0, 1)\n","    '''\n","    Di, idxs = generateDirectSample(arr, m)\n","    Ri = getRandomArray(arr, m)\n","\n","    dists_Di = computeMinDistances(Di, arr, idxs=idxs)\n","    dists_Ri = computeMinDistances(Ri,arr)\n","\n","    dim = arr.shape[1]\n","\n","    Ri_d_norm = np.sum(np.power(dists_Ri, dim))\n","    Di_d_norm = np.sum(np.power(dists_Di, dim))\n","\n","    return Ri_d_norm /(Ri_d_norm + Di_d_norm)"],"execution_count":null,"outputs":[]}]}